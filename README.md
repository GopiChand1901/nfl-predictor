# nfl-predictor

This project is a compact end-to-end pipeline for predicting NFL game winners from public schedule data and a handful of engineered context features. It loads historical schedules with nfl-data-py, converts each game into team-centric rows, and builds “form” statistics that only look into the past (rolling averages of point differential, points for/against, win rate, and rest days). It adds a lightweight Elo system that starts every team at 1500, applies a modest home-field adjustment, and updates ratings after completed games; for modeling, we use the pre-game Elo as a prior and take the home–away difference. Game-level features are then formed by merging the latest home and away side metrics back into each matchup, adding optional market context (spread/total if available), simple environment flags (indoor, grass, wind and temperature thresholds), and a few situational indicators. When any of those inputs are missing, the code falls back to neutral values so training and inference remain robust.

The modeling core is HistGradientBoostingClassifier from scikit-learn, chosen because gradient-boosted trees capture non-linear relationships and interactions far better than linear models, yet keep the dependency footprint small compared with external libraries. HGB also handles missing values natively and is well-behaved on tabular, moderately sized datasets like weekly NFL slates. We tune the model with RandomizedSearchCV over a compact hyperparameter space and, crucially, use TimeSeriesSplit so that each fold respects chronology; this guards against “peeking” into the future, which can silently inflate validation scores in sports data. After hyperparameter search, we wrap the best estimator in isotonic calibration, a non-parametric monotone mapping that corrects raw scores into better-calibrated probabilities. In sports prediction, well-calibrated probabilities matter as much as ranking power: a model that says 0.62 should win about 62% over the long run. To keep cross-validation stable across environments, the search runs under a threading backend rather than process-based parallelism, avoiding joblib/loky pickling issues.

On our held-out 2023 validation season, the gradient-boosting approach achieved an AUC of ~0.79, indicating strong ranking ability (the model tends to score the actual winners higher than the losers nearly 79% of the time). Accuracy at a 0.5 threshold is reported as a convenience, but AUC and probability calibration are the primary quality signals because they’re less sensitive to the weekly base rate and threshold choice. When we compare that long-run behavior to a real-world spot check, the model performed well on September 14, 2025: out of 13 scheduled games, it predicted 10 correctly, which is ~76.9% accuracy for that day. Single-day results will naturally swing—injury surprises, weather shifts, and matchup quirks can move outcomes—but days like this align with the model’s validation profile and give confidence that its ranking and probabilities are meaningful.

The script also includes two user-facing pieces that make it easy to test ideas quickly. First, there’s an interactive CLI where you type a matchup as "Home, Away" (for example, Chiefs, Eagles) and receive calibrated home/away win probabilities plus a small tag indicating favorite strength. Under the hood, this uses a “latest snapshot” of each team’s form and Elo, which is created by running the team-long pipeline across all loaded seasons and grabbing the most recent row per team. Second, if you set a SPORTS_BLAZE_KEY, the script can query the daily NFL schedule and print predictions for today’s games in your local America/Chicago timezone. Both flows rely on the same feature schema and the same saved model object, which is persisted with joblib alongside the feature list; on startup, the code will reload the saved model if and only if the feature schema matches, otherwise it retrains to avoid mismatches that could corrupt inference.

Algorithmically, each choice balances signal and practicality. Rolling and expanding statistics are shifted to ensure they never include the current game, preventing leakage. Elo is intentionally lightweight: it’s a well-worn, transparent prior that stabilizes early-season predictions and helps the model separate teams with similar short-term form. Gradient boosting is favored over baselines like logistic regression because football outcomes depend on interactions (for example, how home-field advantage, rest differential, and recent point differential combine), and HGB can learn those without manual feature crosses. Isotonic calibration is preferred over Platt scaling here because tabular tree ensembles often benefit from a flexible monotone calibrator; the result is probabilities that better track true frequencies across the whole range. Finally, neutral filling and strict numeric coercion make the pipeline resilient when a feed is missing spread, total, or weather, so you can still get sensible predictions rather than fail the run.

There is plenty of room to grow. The current “division game” flag is a crude proxy and will be replaced by a proper division/conference map. Incorporating closing lines and richer team efficiency metrics (EPA, success rate, pressure rate, early-down pass rates, red-zone efficiency) should improve both AUC and calibration, as would ingesting injury/actives data for quarterbacks and key skill positions. A time-decay weighting scheme would let recent games count more without discarding older samples, and adding reliability curves and Brier scores to the evaluation suite would make probability quality explicit alongside AUC. Operationally, the CLI could be split into subcommands (train, predict, today, eval) and exposed through a small FastAPI service for programmatic use.

In short, this codebase turns public NFL schedules into a leak-safe, feature-rich table, fits a tuned and calibrated gradient-boosting model, and serves predictions through both an interactive prompt and an optional daily slate fetch. Its validated AUC around 0.79 and the 10/13 performance snapshot on Sep 14, 2025 suggest the approach ranks teams reliably and produces useful probabilities today, while leaving a clear path to stronger features, better calibration diagnostics, and smoother deployment
